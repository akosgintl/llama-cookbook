# Configuration for data loading, formatting, and fine-tuning

# Data source configuration
data_path: "your/dataset/path"  # Path to the dataset (either a Hugging Face dataset ID or a local path)
is_local: true                  # Whether the data is stored locally

# Formatter configuration
formatter_type: "torchtune"     # Type of formatter to use ('torchtune', 'vllm', or 'openai')

# Column mapping configuration
# Maps custom column names to standard field names
column_mapping:
  input: "question"             # Field containing the input text
  output: "answer"              # Field containing the output text
  image: "image_path"           # Field containing the image path (optional)

# Additional arguments to pass to the load_dataset function
dataset_kwargs:
  split: "train"                # Dataset split to load
  # Add any other dataset-specific arguments here

# Training configuration
finetuning:
  strategy: "lora"               # Training strategy ('fft' or 'lora')
  num_epochs: 1                 # Number of training epochs
  batch_size: 1                 # Batch size per device for training
  torchtune_config: "llama3_2_vision/11B_lora"             # TorchTune-specific configuration
  num_processes_per_node: 8             # TorchTune-specific configuration
  distributed: true             # Whether to use distributed training