{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdDXB8ivjXwB"
      },
      "source": [
        "# Finetuning Llama-3.1 8b with text2sql SFT dataset\n",
        "\n",
        "This notebook implements necessary data transformations and installation of libraries. It works with Google Colab A-100 GPU (paid) instances. There are comments for changing it into int4 quantizations on T4 GPU.\n",
        "\n",
        "Unsloth library and code is used to lower GPU memory requirement and increase performance. See https://github.com/unslothai/unsloth\n",
        "\n",
        "To upload training dataset, click on Folder Icon on the left hand side of your Colab Notebook and upload your training data file: train_text2sql_sft_dataset.json (see instructions for generating the dataset under \"finetuning\" parent directory)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Click on Folder icon on the left hand side of your Colab Notebook and upload your training data file: train_text2sql_sft_dataset.json\n",
        "training_data_file = \"train_text2sql_sft_dataset.json\""
      ],
      "metadata": {
        "id": "LNDhsv0kfBih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_K2EX0ejXwC"
      },
      "outputs": [],
      "source": [
        "# install libraries\n",
        "!pip install -U datasets\n",
        "\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "08cbd090cd594a998fc641cf4880df55",
            "cfad6b5f4ce5418ba6555668de71fc5e",
            "c9797e0a5d1746ffa9719278b8c6df97",
            "bca3230d0f5841348e1d52bb965eb88a",
            "eb24a737e7f949bbba05c97fc3930509",
            "c1e4dbfa522e432cadaae898dc38b6d2",
            "98a307da59ce474d83c078c4969e5bba",
            "2e9a5b5f7bfc4205a2b7fc4d07527127",
            "d8ff81bca7e44192bfc610cfebc78a99",
            "fba7465393574fc4a188a0126dc42c63",
            "a4215bedeb224a37a40edf231308c247"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "41922c6c-825f-4cf8-e857-d3dda55050c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.6.8: Fast Llama patching. Transformers: 4.52.4.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08cbd090cd594a998fc641cf4880df55"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# Unsloth 4bit models\n",
        "# fourbit_models = [\n",
        "#     \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "#     \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "#     \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "#     \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",\n",
        "# ]\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\", # changed to \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\" and load_in_4bit= True for 4bit\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"You are a text to SQL query translator. Using the SQLite DB Schema and the External Knowledge, translate the following text question into a SQLite SQL select statement.\", # instruction\n",
        "        \"-- DB Schema: CREATE TABLE \\\"lists\\\"\\n(\\n    user_id                     INTEGER\\n        references lists_users (user_id),\\n    list_id                     INTEGER not null\\n        primary key,\\n    list_title                  TEXT,\\n    list_movie_number           INTEGER,\\n    list_update_timestamp_utc   TEXT,\\n    list_creation_timestamp_utc TEXT,\\n    list_followers              INTEGER,\\n    list_url                    TEXT,\\n    list_comments               INTEGER,\\n    list_description            TEXT,\\n    list_cover_image_url        TEXT,\\n    list_first_image_url        TEXT,\\n    list_second_image_url       TEXT,\\n    list_third_image_url        TEXT\\n)\\n\\nCREATE TABLE \\\"movies\\\"\\n(\\n    movie_id             INTEGER not null\\n        primary key,\\n    movie_title          TEXT,\\n    movie_release_year   INTEGER,\\n    movie_url            TEXT,\\n    movie_title_language TEXT,\\n    movie_popularity     INTEGER,\\n    movie_image_url      TEXT,\\n    director_id          TEXT,\\n    director_name        TEXT,\\n    director_url         TEXT\\n)\\n\\nCREATE TABLE \\\"ratings_users\\\"\\n(\\n    user_id                 INTEGER\\n        references lists_users (user_id),\\n    rating_date_utc         TEXT,\\n    user_trialist           INTEGER,\\n    user_subscriber         INTEGER,\\n    user_avatar_image_url   TEXT,\\n    user_cover_image_url    TEXT,\\n    user_eligible_for_trial INTEGER,\\n    user_has_payment_method INTEGER\\n)\\n\\nCREATE TABLE lists_users\\n(\\n    user_id                 INTEGER not null ,\\n    list_id                 INTEGER not null ,\\n    list_update_date_utc    TEXT,\\n    list_creation_date_utc  TEXT,\\n    user_trialist           INTEGER,\\n    user_subscriber         INTEGER,\\n    user_avatar_image_url   TEXT,\\n    user_cover_image_url    TEXT,\\n    user_eligible_for_trial TEXT,\\n    user_has_payment_method TEXT,\\n    primary key (user_id, list_id),\\n    foreign key (list_id) references lists(list_id),\\n    foreign key (user_id) references lists(user_id)\\n)\\n\\nCREATE TABLE ratings\\n(\\n    movie_id                INTEGER,\\n    rating_id               INTEGER,\\n    rating_url              TEXT,\\n    rating_score            INTEGER,\\n    rating_timestamp_utc    TEXT,\\n    critic                  TEXT,\\n    critic_likes            INTEGER,\\n    critic_comments         INTEGER,\\n    user_id                 INTEGER,\\n    user_trialist           INTEGER,\\n    user_subscriber         INTEGER,\\n    user_eligible_for_trial INTEGER,\\n    user_has_payment_method INTEGER,\\n    foreign key (movie_id) references movies(movie_id),\\n    foreign key (user_id) references lists_users(user_id),\\n    foreign key (rating_id) references ratings(rating_id),\\n    foreign key (user_id) references ratings_users(user_id)\\n)\\n\\n-- External Knowledge: longest movie title refers to MAX(LENGTH(movie_title)); when it was released refers to movie_release_year;\\n\\n-- Question: What is the name of the longest movie title? When was it released?\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    ) + EOS_TOKEN\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9VhRGLm7bZH",
        "outputId": "696647fb-7f5b-4548-b089-ed54ab5c3e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are a text to SQL query translator. Using the SQLite DB Schema and the External Knowledge, translate the following text question into a SQLite SQL select statement.\n",
            "\n",
            "### Input:\n",
            "-- DB Schema: CREATE TABLE \"lists\"\n",
            "(\n",
            "    user_id                     INTEGER\n",
            "        references lists_users (user_id),\n",
            "    list_id                     INTEGER not null\n",
            "        primary key,\n",
            "    list_title                  TEXT,\n",
            "    list_movie_number           INTEGER,\n",
            "    list_update_timestamp_utc   TEXT,\n",
            "    list_creation_timestamp_utc TEXT,\n",
            "    list_followers              INTEGER,\n",
            "    list_url                    TEXT,\n",
            "    list_comments               INTEGER,\n",
            "    list_description            TEXT,\n",
            "    list_cover_image_url        TEXT,\n",
            "    list_first_image_url        TEXT,\n",
            "    list_second_image_url       TEXT,\n",
            "    list_third_image_url        TEXT\n",
            ")\n",
            "\n",
            "CREATE TABLE \"movies\"\n",
            "(\n",
            "    movie_id             INTEGER not null\n",
            "        primary key,\n",
            "    movie_title          TEXT,\n",
            "    movie_release_year   INTEGER,\n",
            "    movie_url            TEXT,\n",
            "    movie_title_language TEXT,\n",
            "    movie_popularity     INTEGER,\n",
            "    movie_image_url      TEXT,\n",
            "    director_id          TEXT,\n",
            "    director_name        TEXT,\n",
            "    director_url         TEXT\n",
            ")\n",
            "\n",
            "CREATE TABLE \"ratings_users\"\n",
            "(\n",
            "    user_id                 INTEGER\n",
            "        references lists_users (user_id),\n",
            "    rating_date_utc         TEXT,\n",
            "    user_trialist           INTEGER,\n",
            "    user_subscriber         INTEGER,\n",
            "    user_avatar_image_url   TEXT,\n",
            "    user_cover_image_url    TEXT,\n",
            "    user_eligible_for_trial INTEGER,\n",
            "    user_has_payment_method INTEGER\n",
            ")\n",
            "\n",
            "CREATE TABLE lists_users\n",
            "(\n",
            "    user_id                 INTEGER not null,\n",
            "    list_id                 INTEGER not null,\n",
            "    list_update_date_utc    TEXT,\n",
            "    list_creation_date_utc  TEXT,\n",
            "    user_trialist           INTEGER,\n",
            "    user_subscriber         INTEGER,\n",
            "    user_avatar_image_url   TEXT,\n",
            "    user_cover_image_url    TEXT,\n",
            "    user_eligible_for_trial TEXT,\n",
            "    user_has_payment_method TEXT,\n",
            "    primary key (user_id, list_id),\n",
            "    foreign key (list_id) references lists(list_id),\n",
            "    foreign key (user_id) references lists(user_id)\n",
            ")\n",
            "\n",
            "CREATE TABLE ratings\n",
            "(\n",
            "    movie_id                INTEGER,\n",
            "    rating_id               INTEGER,\n",
            "    rating_url              TEXT,\n",
            "    rating_score            INTEGER,\n",
            "    rating_timestamp_utc    TEXT,\n",
            "    critic                  TEXT,\n",
            "    critic_likes            INTEGER,\n",
            "    critic_comments         INTEGER,\n",
            "    user_id                 INTEGER,\n",
            "    user_trialist           INTEGER,\n",
            "    user_subscriber         INTEGER,\n",
            "    user_eligible_for_trial INTEGER,\n",
            "    user_has_payment_method INTEGER,\n",
            "    foreign key (movie_id) references movies(movie_id),\n",
            "    foreign key (user_id) references lists_users(user_id),\n",
            "    foreign key (rating_id) references ratings(rating_id),\n",
            "    foreign key (user_id) references ratings_users(user_id)\n",
            ")\n",
            "\n",
            "-- External Knowledge: longest movie title refers to MAX(LENGTH(movie_title)); when it was released refers to movie_release_year;\n",
            "\n",
            "-- Question: What is the name of the longest movie title? When was it released?\n",
            "\n",
            "### Response:\n",
            "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "SELECT movie_title, movie_release_year FROM movies ORDER BY LENGTH(movie_title) DESC LIMIT 1;<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "1473bfc7-ed18-4ad5-a115-b2872c7d5f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.6.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Prep\n",
        "# We transform data from existing .json format into new prompt format.\n",
        "\n",
        "# **[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "lines = []\n",
        "with open(training_data_file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "print(lines[:2])\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(messages):\n",
        "    instruction = messages['messages'][0][\"content\"]\n",
        "    input       = messages['messages'][1][\"content\"]\n",
        "    output      = messages['messages'][2][\"content\"]\n",
        "    text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "    return { \"text\" : text, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset, load_from_disk\n",
        "dataset_raw = load_dataset(\"json\", data_files=\"train_text2sql_sft_dataset.json\", split=\"train\")\n",
        "dataset = dataset_raw.map(formatting_prompts_func, batched = False,)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "637dedfe81724192b863c2b131aeb933",
            "781541b6bc7c473ea2cf33f83017b648",
            "c36068a9af284e31890f576d50da2c5f",
            "c3930523cc43497dbcd21eba69dc3b6d",
            "32c851dd54954b72950a9e0f06d01aa1",
            "49e949498a004d2ca5f06edd67304327",
            "70f55e99c7944da9ae8a6445a0ce9b6b",
            "4c13c8d1c64e4d1893ebcc7712907951",
            "8ad04e1707b74a0b89e1aa7e511584e0",
            "4250701d8d3849feb9d00e94f259d980",
            "91e6c3a9ea524d5ab7b7c8fce66d7a36",
            "61ad4905a7c448fea70755c44a9ee7f3",
            "9dbc55420dd349f79b4fcbebe9cb98bd",
            "2d8b27dd71884459ace7a252ff86603f",
            "f505fbfb17064ba59d2bfc6ff4bc41b2",
            "446fd4d425a840799c06c1bae0ab133b",
            "95bae4f5e1424f1480aec8d53490bba4",
            "6223f835a0164b6ea52b1889b0bcd0f8",
            "b5a0439f345d4e6ba6f18f9ac9dfe771",
            "cce40322aa7f48d6abfa03506d0e2630",
            "42581a1a1e364903974416b11989f0f6",
            "29a85fb494734faca4baf1ecb00f0c3c"
          ]
        },
        "id": "rruFkJLzmBp8",
        "outputId": "b340cf82-3d91-4270-8e5c-584804c3186f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['{\"messages\":[{\"content\":\"You are a text to SQL query translator. Using the SQLite DB Schema and the External Knowledge, translate the following text question into a SQLite SQL select statement.\",\"role\":\"system\"},{\"content\":\"-- DB Schema: CREATE TABLE \\\\\"lists\\\\\"\\\\n(\\\\n    user_id                     INTEGER\\\\n        references lists_users (user_id),\\\\n    list_id                     INTEGER not null\\\\n        primary key,\\\\n    list_title                  TEXT,\\\\n    list_movie_number           INTEGER,\\\\n    list_update_timestamp_utc   TEXT,\\\\n    list_creation_timestamp_utc TEXT,\\\\n    list_followers              INTEGER,\\\\n    list_url                    TEXT,\\\\n    list_comments               INTEGER,\\\\n    list_description            TEXT,\\\\n    list_cover_image_url        TEXT,\\\\n    list_first_image_url        TEXT,\\\\n    list_second_image_url       TEXT,\\\\n    list_third_image_url        TEXT\\\\n)\\\\n\\\\nCREATE TABLE \\\\\"movies\\\\\"\\\\n(\\\\n    movie_id             INTEGER not null\\\\n        primary key,\\\\n    movie_title          TEXT,\\\\n    movie_release_year   INTEGER,\\\\n    movie_url            TEXT,\\\\n    movie_title_language TEXT,\\\\n    movie_popularity     INTEGER,\\\\n    movie_image_url      TEXT,\\\\n    director_id          TEXT,\\\\n    director_name        TEXT,\\\\n    director_url         TEXT\\\\n)\\\\n\\\\nCREATE TABLE \\\\\"ratings_users\\\\\"\\\\n(\\\\n    user_id                 INTEGER\\\\n        references lists_users (user_id),\\\\n    rating_date_utc         TEXT,\\\\n    user_trialist           INTEGER,\\\\n    user_subscriber         INTEGER,\\\\n    user_avatar_image_url   TEXT,\\\\n    user_cover_image_url    TEXT,\\\\n    user_eligible_for_trial INTEGER,\\\\n    user_has_payment_method INTEGER\\\\n)\\\\n\\\\nCREATE TABLE lists_users\\\\n(\\\\n    user_id                 INTEGER not null ,\\\\n    list_id                 INTEGER not null ,\\\\n    list_update_date_utc    TEXT,\\\\n    list_creation_date_utc  TEXT,\\\\n    user_trialist           INTEGER,\\\\n    user_subscriber         INTEGER,\\\\n    user_avatar_image_url   TEXT,\\\\n    user_cover_image_url    TEXT,\\\\n    user_eligible_for_trial TEXT,\\\\n    user_has_payment_method TEXT,\\\\n    primary key (user_id, list_id),\\\\n    foreign key (list_id) references lists(list_id),\\\\n    foreign key (user_id) references lists(user_id)\\\\n)\\\\n\\\\nCREATE TABLE ratings\\\\n(\\\\n    movie_id                INTEGER,\\\\n    rating_id               INTEGER,\\\\n    rating_url              TEXT,\\\\n    rating_score            INTEGER,\\\\n    rating_timestamp_utc    TEXT,\\\\n    critic                  TEXT,\\\\n    critic_likes            INTEGER,\\\\n    critic_comments         INTEGER,\\\\n    user_id                 INTEGER,\\\\n    user_trialist           INTEGER,\\\\n    user_subscriber         INTEGER,\\\\n    user_eligible_for_trial INTEGER,\\\\n    user_has_payment_method INTEGER,\\\\n    foreign key (movie_id) references movies(movie_id),\\\\n    foreign key (user_id) references lists_users(user_id),\\\\n    foreign key (rating_id) references ratings(rating_id),\\\\n    foreign key (user_id) references ratings_users(user_id)\\\\n)\\\\n\\\\n-- External Knowledge: released in the year 1945 refers to movie_release_year = 1945;\\\\n\\\\n-- Question: Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.\",\"role\":\"user\"},{\"content\":\"SELECT movie_title FROM movies WHERE movie_release_year = 1945 ORDER BY movie_popularity DESC LIMIT 1\",\"role\":\"assistant\"}]}\\n', '{\"messages\":[{\"content\":\"You are a text to SQL query translator. Using the SQLite DB Schema and the External Knowledge, translate the following text question into a SQLite SQL select statement.\",\"role\":\"system\"},{\"content\":\"-- DB Schema: CREATE TABLE \\\\\"lists\\\\\"\\\\n(\\\\n    user_id                     INTEGER\\\\n        references lists_users (user_id),\\\\n    list_id                     INTEGER not null\\\\n        primary key,\\\\n    list_title                  TEXT,\\\\n    list_movie_number           INTEGER,\\\\n    list_update_timestamp_utc   TEXT,\\\\n    list_creation_timestamp_utc TEXT,\\\\n    list_followers              INTEGER,\\\\n    list_url                    TEXT,\\\\n    list_comments               INTEGER,\\\\n    list_description            TEXT,\\\\n    list_cover_image_url        TEXT,\\\\n    list_first_image_url        TEXT,\\\\n    list_second_image_url       TEXT,\\\\n    list_third_image_url        TEXT\\\\n)\\\\n\\\\nCREATE TABLE \\\\\"movies\\\\\"\\\\n(\\\\n    movie_id             INTEGER not null\\\\n        primary key,\\\\n    movie_title          TEXT,\\\\n    movie_release_year   INTEGER,\\\\n    movie_url            TEXT,\\\\n    movie_title_language TEXT,\\\\n    movie_popularity     INTEGER,\\\\n    movie_image_url      TEXT,\\\\n    director_id          TEXT,\\\\n    director_name        TEXT,\\\\n    director_url         TEXT\\\\n)\\\\n\\\\nCREATE TABLE \\\\\"ratings_users\\\\\"\\\\n(\\\\n    user_id                 INTEGER\\\\n        references lists_users (user_id),\\\\n    rating_date_utc         TEXT,\\\\n    user_trialist           INTEGER,\\\\n    user_subscriber         INTEGER,\\\\n    user_avatar_image_url   TEXT,\\\\n    user_cover_image_url    TEXT,\\\\n    user_eligible_for_trial INTEGER,\\\\n    user_has_payment_method INTEGER\\\\n)\\\\n\\\\nCREATE TABLE lists_users\\\\n(\\\\n    user_id                 INTEGER not null ,\\\\n    list_id                 INTEGER not null ,\\\\n    list_update_date_utc    TEXT,\\\\n    list_creation_date_utc  TEXT,\\\\n    user_trialist           INTEGER,\\\\n    user_subscriber         INTEGER,\\\\n    user_avatar_image_url   TEXT,\\\\n    user_cover_image_url    TEXT,\\\\n    user_eligible_for_trial TEXT,\\\\n    user_has_payment_method TEXT,\\\\n    primary key (user_id, list_id),\\\\n    foreign key (list_id) references lists(list_id),\\\\n    foreign key (user_id) references lists(user_id)\\\\n)\\\\n\\\\nCREATE TABLE ratings\\\\n(\\\\n    movie_id                INTEGER,\\\\n    rating_id               INTEGER,\\\\n    rating_url              TEXT,\\\\n    rating_score            INTEGER,\\\\n    rating_timestamp_utc    TEXT,\\\\n    critic                  TEXT,\\\\n    critic_likes            INTEGER,\\\\n    critic_comments         INTEGER,\\\\n    user_id                 INTEGER,\\\\n    user_trialist           INTEGER,\\\\n    user_subscriber         INTEGER,\\\\n    user_eligible_for_trial INTEGER,\\\\n    user_has_payment_method INTEGER,\\\\n    foreign key (movie_id) references movies(movie_id),\\\\n    foreign key (user_id) references lists_users(user_id),\\\\n    foreign key (rating_id) references ratings(rating_id),\\\\n    foreign key (user_id) references ratings_users(user_id)\\\\n)\\\\n\\\\n-- External Knowledge: most popular movie refers to MAX(movie_popularity); when it was released refers to movie_release_year; director for the movie refers to director_name;\\\\n\\\\n-- Question: State the most popular movie? When was it released and who is the director for the movie?\",\"role\":\"user\"},{\"content\":\"SELECT movie_title, movie_release_year, director_name FROM movies ORDER BY movie_popularity DESC LIMIT 1 \",\"role\":\"assistant\"}]}\\n']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "637dedfe81724192b863c2b131aeb933"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/6599 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61ad4905a7c448fea70755c44a9ee7f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['messages', 'text'],\n",
              "    num_rows: 6599\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1e212fe262544328967e36fb26e06073",
            "0badc1c8b9424b8cb2c526a3bc80294a",
            "d0c3e1bc46314b3a822e8350f6ed4904",
            "46538c8c222a4ebb81abf0daad5bfa79",
            "52711139b56948fcbeca651623d194a0",
            "78a35430e1bd47549124eac98be01fdd",
            "701bbc75e7214abc93e7d15d42f7c22f",
            "09d61405fbb4468cb49a430b6b462276",
            "4e36e192ac3947efbb9a8a4e2f4cfbe2",
            "a2e091222faa4d38877b613f8036025d",
            "7d56d8683551408182969f6201605c86"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "49490f23-f927-424b-8e88-2b77b0f18aed"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/6599 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e212fe262544328967e36fb26e06073"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\", # Explicitly set the text field\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"tensorboard\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "4a2ca826-e796-4e8c-aade-caa2081a7ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
            "30.502 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "9dc852a4-daa7-4f4c-d575-0c5a2bb0161b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 6,599 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040/8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 02:09, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.124300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.146600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.080300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.007300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.294600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.990800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.945300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.977200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.709900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.730600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.742400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.651700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.610100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.659700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.582100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.526100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.701700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.540800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.580600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.466500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.432900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.588300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.392800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.571500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.473200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.432700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.460100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.510300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.329800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.448200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.444500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.344200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.439600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.302700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.403300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.348200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.363500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.398100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.518900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.263000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.286800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.320400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.324000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.387200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.230200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.448800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.196300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.384200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.337900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.259800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.262400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.360300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.151700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.220300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.297200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.238500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.292100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.321500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.257700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.212100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "26677bc3-d541-49ff-fc57-6db2b956e0f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "144.3049 seconds used for training.\n",
            "2.41 minutes used for training.\n",
            "Peak reserved memory = 30.502 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 77.109 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PGU_6HO40Q2i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "08b36347-9bcc-40db-e9cc-f036668f0727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nYou are a text to SQL query translator. Using the SQLite DB Schema and the External Knowledge, translate the following text question into a SQLite SQL select statement.\\n\\n### Input:\\n-- DB Schema: CREATE TABLE \"lists\"\\n(\\n    user_id                     INTEGER\\n        references lists_users (user_id),\\n    list_id                     INTEGER not null\\n        primary key,\\n    list_title                  TEXT,\\n    list_movie_number           INTEGER,\\n    list_update_timestamp_utc   TEXT,\\n    list_creation_timestamp_utc TEXT,\\n    list_followers              INTEGER,\\n    list_url                    TEXT,\\n    list_comments               INTEGER,\\n    list_description            TEXT,\\n    list_cover_image_url        TEXT,\\n    list_first_image_url        TEXT,\\n    list_second_image_url       TEXT,\\n    list_third_image_url        TEXT\\n)\\n\\nCREATE TABLE \"movies\"\\n(\\n    movie_id             INTEGER not null\\n        primary key,\\n    movie_title          TEXT,\\n    movie_release_year   INTEGER,\\n    movie_url            TEXT,\\n    movie_title_language TEXT,\\n    movie_popularity     INTEGER,\\n    movie_image_url      TEXT,\\n    director_id          TEXT,\\n    director_name        TEXT,\\n    director_url         TEXT\\n)\\n\\nCREATE TABLE \"ratings_users\"\\n(\\n    user_id                 INTEGER\\n        references lists_users (user_id),\\n    rating_date_utc         TEXT,\\n    user_trialist           INTEGER,\\n    user_subscriber         INTEGER,\\n    user_avatar_image_url   TEXT,\\n    user_cover_image_url    TEXT,\\n    user_eligible_for_trial INTEGER,\\n    user_has_payment_method INTEGER\\n)\\n\\nCREATE TABLE lists_users\\n(\\n    user_id                 INTEGER not null,\\n    list_id                 INTEGER not null,\\n    list_update_date_utc    TEXT,\\n    list_creation_date_utc  TEXT,\\n    user_trialist           INTEGER,\\n    user_subscriber         INTEGER,\\n    user_avatar_image_url   TEXT,\\n    user_cover_image_url    TEXT,\\n    user_eligible_for_trial TEXT,\\n    user_has_payment_method TEXT,\\n    primary key (user_id, list_id),\\n    foreign key (list_id) references lists(list_id),\\n    foreign key (user_id) references lists(user_id)\\n)\\n\\nCREATE TABLE ratings\\n(\\n    movie_id                INTEGER,\\n    rating_id               INTEGER,\\n    rating_url              TEXT,\\n    rating_score            INTEGER,\\n    rating_timestamp_utc    TEXT,\\n    critic                  TEXT,\\n    critic_likes            INTEGER,\\n    critic_comments         INTEGER,\\n    user_id                 INTEGER,\\n    user_trialist           INTEGER,\\n    user_subscriber         INTEGER,\\n    user_eligible_for_trial INTEGER,\\n    user_has_payment_method INTEGER,\\n    foreign key (movie_id) references movies(movie_id),\\n    foreign key (user_id) references lists_users(user_id),\\n    foreign key (rating_id) references ratings(rating_id),\\n    foreign key (user_id) references ratings_users(user_id)\\n)\\n\\n-- External Knowledge: longest movie title refers to MAX(LENGTH(movie_title)); when it was released refers to movie_release_year;\\n\\n-- Question: What is the name of the longest movie title? When was it released?\\n\\n### Response:\\nSELECT T2.movie_title, T2.movie_release_year FROM lists AS T1 INNER JOIN movies AS T2 ON T1.list_id = T2.movie_id ORDER BY LENGTH(T2.movie_title) DESC LIMIT 1<|eot_id|>']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"You are a text to SQL query translator. Using the SQLite DB Schema and the External Knowledge, translate the following text question into a SQLite SQL select statement.\", # instruction\n",
        "        \"-- DB Schema: CREATE TABLE \\\"lists\\\"\\n(\\n    user_id                     INTEGER\\n        references lists_users (user_id),\\n    list_id                     INTEGER not null\\n        primary key,\\n    list_title                  TEXT,\\n    list_movie_number           INTEGER,\\n    list_update_timestamp_utc   TEXT,\\n    list_creation_timestamp_utc TEXT,\\n    list_followers              INTEGER,\\n    list_url                    TEXT,\\n    list_comments               INTEGER,\\n    list_description            TEXT,\\n    list_cover_image_url        TEXT,\\n    list_first_image_url        TEXT,\\n    list_second_image_url       TEXT,\\n    list_third_image_url        TEXT\\n)\\n\\nCREATE TABLE \\\"movies\\\"\\n(\\n    movie_id             INTEGER not null\\n        primary key,\\n    movie_title          TEXT,\\n    movie_release_year   INTEGER,\\n    movie_url            TEXT,\\n    movie_title_language TEXT,\\n    movie_popularity     INTEGER,\\n    movie_image_url      TEXT,\\n    director_id          TEXT,\\n    director_name        TEXT,\\n    director_url         TEXT\\n)\\n\\nCREATE TABLE \\\"ratings_users\\\"\\n(\\n    user_id                 INTEGER\\n        references lists_users (user_id),\\n    rating_date_utc         TEXT,\\n    user_trialist           INTEGER,\\n    user_subscriber         INTEGER,\\n    user_avatar_image_url   TEXT,\\n    user_cover_image_url    TEXT,\\n    user_eligible_for_trial INTEGER,\\n    user_has_payment_method INTEGER\\n)\\n\\nCREATE TABLE lists_users\\n(\\n    user_id                 INTEGER not null ,\\n    list_id                 INTEGER not null ,\\n    list_update_date_utc    TEXT,\\n    list_creation_date_utc  TEXT,\\n    user_trialist           INTEGER,\\n    user_subscriber         INTEGER,\\n    user_avatar_image_url   TEXT,\\n    user_cover_image_url    TEXT,\\n    user_eligible_for_trial TEXT,\\n    user_has_payment_method TEXT,\\n    primary key (user_id, list_id),\\n    foreign key (list_id) references lists(list_id),\\n    foreign key (user_id) references lists(user_id)\\n)\\n\\nCREATE TABLE ratings\\n(\\n    movie_id                INTEGER,\\n    rating_id               INTEGER,\\n    rating_url              TEXT,\\n    rating_score            INTEGER,\\n    rating_timestamp_utc    TEXT,\\n    critic                  TEXT,\\n    critic_likes            INTEGER,\\n    critic_comments         INTEGER,\\n    user_id                 INTEGER,\\n    user_trialist           INTEGER,\\n    user_subscriber         INTEGER,\\n    user_eligible_for_trial INTEGER,\\n    user_has_payment_method INTEGER,\\n    foreign key (movie_id) references movies(movie_id),\\n    foreign key (user_id) references lists_users(user_id),\\n    foreign key (rating_id) references ratings(rating_id),\\n    foreign key (user_id) references ratings_users(user_id)\\n)\\n\\n-- External Knowledge: longest movie title refers to MAX(LENGTH(movie_title)); when it was released refers to movie_release_year;\\n\\n-- Question: What is the name of the longest movie title? When was it released?\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = False)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # alpaca_prompt = Copied from above\n",
        "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "# inputs = tokenizer(\n",
        "# [\n",
        "#     alpaca_prompt.format(\n",
        "#         \"You are a text to SQL query translator. Using the SQLite DB Schema and the External Knowledge, translate the following text question into a SQLite SQL select statement.\", # instruction\n",
        "#         \"-- DB Schema: CREATE TABLE \\\"lists\\\"\\n(\\n    user_id                     INTEGER\\n        references lists_users (user_id),\\n    list_id                     INTEGER not null\\n        primary key,\\n    list_title                  TEXT,\\n    list_movie_number           INTEGER,\\n    list_update_timestamp_utc   TEXT,\\n    list_creation_timestamp_utc TEXT,\\n    list_followers              INTEGER,\\n    list_url                    TEXT,\\n    list_comments               INTEGER,\\n    list_description            TEXT,\\n    list_cover_image_url        TEXT,\\n    list_first_image_url        TEXT,\\n    list_second_image_url       TEXT,\\n    list_third_image_url        TEXT\\n)\\n\\nCREATE TABLE \\\"movies\\\"\\n(\\n    movie_id             INTEGER not null\\n        primary key,\\n    movie_title          TEXT,\\n    movie_release_year   INTEGER,\\n    movie_url            TEXT,\\n    movie_title_language TEXT,\\n    movie_popularity     INTEGER,\\n    movie_image_url      TEXT,\\n    director_id          TEXT,\\n    director_name        TEXT,\\n    director_url         TEXT\\n)\\n\\nCREATE TABLE \\\"ratings_users\\\"\\n(\\n    user_id                 INTEGER\\n        references lists_users (user_id),\\n    rating_date_utc         TEXT,\\n    user_trialist           INTEGER,\\n    user_subscriber         INTEGER,\\n    user_avatar_image_url   TEXT,\\n    user_cover_image_url    TEXT,\\n    user_eligible_for_trial INTEGER,\\n    user_has_payment_method INTEGER\\n)\\n\\nCREATE TABLE lists_users\\n(\\n    user_id                 INTEGER not null ,\\n    list_id                 INTEGER not null ,\\n    list_update_date_utc    TEXT,\\n    list_creation_date_utc  TEXT,\\n    user_trialist           INTEGER,\\n    user_subscriber         INTEGER,\\n    user_avatar_image_url   TEXT,\\n    user_cover_image_url    TEXT,\\n    user_eligible_for_trial TEXT,\\n    user_has_payment_method TEXT,\\n    primary key (user_id, list_id),\\n    foreign key (list_id) references lists(list_id),\\n    foreign key (user_id) references lists(user_id)\\n)\\n\\nCREATE TABLE ratings\\n(\\n    movie_id                INTEGER,\\n    rating_id               INTEGER,\\n    rating_url              TEXT,\\n    rating_score            INTEGER,\\n    rating_timestamp_utc    TEXT,\\n    critic                  TEXT,\\n    critic_likes            INTEGER,\\n    critic_comments         INTEGER,\\n    user_id                 INTEGER,\\n    user_trialist           INTEGER,\\n    user_subscriber         INTEGER,\\n    user_eligible_for_trial INTEGER,\\n    user_has_payment_method INTEGER,\\n    foreign key (movie_id) references movies(movie_id),\\n    foreign key (user_id) references lists_users(user_id),\\n    foreign key (rating_id) references ratings(rating_id),\\n    foreign key (user_id) references ratings_users(user_id)\\n)\\n\\n-- External Knowledge: longest movie title refers to MAX(LENGTH(movie_title)); when it was released refers to movie_release_year;\\n\\n-- Question: What is the name of the longest movie title? When was it released?\", # input\n",
        "#         \"\", # output - leave this blank for generation!\n",
        "#     )\n",
        "# ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n",
        "# tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "dusmu3Y-WPl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "b89abd16ac7645c2af3861be2a53e7a4",
            "0a3dbf62929640f5b670bb9932ff9e85",
            "774bc595ccdf4fcfbc078ae69e31cce2",
            "a401420ba5a84c4e8e09d189ae31791f",
            "d9338d0b5aa14d98a230f64a96dacfc6",
            "da237a8e84234e9ab6eb73c8cc41dac5",
            "6b730c841c28487596cb1358402f618b",
            "954b8f5078384488b503919bb47b380e",
            "2e589652d1b641b1bd49109f3a963a63",
            "9db7e5022a1a4c82a4b218c4f0e58a7d",
            "ec69d72c400d497cb43db1a2c4d3ffd0",
            "9124002ea80946079b05669b8edf2e93",
            "2418f4815f8e4db38821f04715509a87",
            "8b024ae17b3d4f9d9458e0d24c4ba858",
            "3aca95703e0b4da195020b22e03794d5",
            "888e1a411dd84e68a9fb45319a384e53",
            "2aa533d9f8154a2890442924f939b936",
            "f7f0f1b891454fcb8271c2fd9afa5e39",
            "032982a7372d47c09a039de0f89e7626",
            "e909536897824b8492ed21651d7cdcc2",
            "56f54ec10c5a411d97a8add4e37419e7",
            "9ae2b77ef66949a8ac1a2c073adf654e",
            "be77d2c16a8544ffae3c3224a88ceac7",
            "18730cc628544463be32a527143908e7",
            "783f0b6997c04133a92e75c8c1970555",
            "317c60b6371b4f0b8903659a48915549",
            "3299e112ee0e4b948383d0965e000665",
            "123983553f524c86b2e34137c3e15a81",
            "f8f4c7d3484040f5b7cc63dd507e7cac",
            "bb9630f74a664507ac5fa5e7c3b28287",
            "b80165d852f547f28d0ef91276586a4d",
            "a46d09b6a9b649bd95b48d1ec9430a55",
            "358661cd13c14e6eb64591b288f9f02f"
          ]
        },
        "outputId": "50f44657-a02d-4657-ce10-29436f4a4ad8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/587 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b89abd16ac7645c2af3861be2a53e7a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading...:   0%|          | 0.00/168M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9124002ea80946079b05669b8edf2e93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to https://huggingface.co/amiryo/Meta-Llama-3.1-8B-Instruct-lora-only\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading...:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be77d2c16a8544ffae3c3224a88ceac7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Uncomment for saving Adaptor Only. We save marged model in a future step.\n",
        "\n",
        "#model.save_pretrained(\"Meta-Llama-3.1-8B-Instruct_lora_model\")  # Local saving\n",
        "#tokenizer.save_pretrained(\"Meta-Llama-3.1-8B-Instruct_lora_model\")\n",
        "\n",
        "#model.push_to_hub(\"amiryo/Meta-Llama-3.1-8B-Instruct-lora-only\", token = \"...\") # Online saving\n",
        "#tokenizer.push_to_hub(\"amiryo/Meta-Llama-3.1-8B-Instruct-lora-only\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "\n",
        "# We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.\n",
        "\n",
        "# Merge to 16bit. Saving to float16 for VLLM etc.\n",
        "if False: model.push_to_hub_merged(\"your_hf/Meta-Llama-3.1-8B-Instruct-merged-16bit\", tokenizer, save_method = \"merged_16bit\", token = \"hf_...\")\n",
        "if False: model.save_pretrained_merged(\"Meta-Llama-3.1-8B-Instruct-merged-16bit\", tokenizer, save_method = \"merged_16bit\",)\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.push_to_hub_merged(\"your_hf/Meta-Llama-3.1-8B-Instruct-lora\", tokenizer, save_method = \"lora\", token = \"hf_...\")\n",
        "if False: model.save_pretrained_merged(\"Meta-Llama-3.1-8B-Instruct-lora\", tokenizer, save_method = \"lora\",)\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}